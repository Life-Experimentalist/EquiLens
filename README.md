# ğŸ¯ EquiLens - AI Bias Detection Platform

**Professional AI bias detection platform with interactive CLI, GPU acceleration, and comprehensive reporting**

| License                                                                         | Python                                                                    | Docker                                                                       | Platform                                                                                                             |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| [![License](https://img.shields.io/badge/license-Apache%202.0-green)](#license) | [![Python](https://img.shields.io/badge/python-3.13-blue)](#requirements) | [![Docker](https://img.shields.io/badge/docker-compose-blue)](#docker-setup) | [![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-brightgreen)](#compatibility) |

## ğŸŒŸ Key Features

- ğŸ¨ **Interactive CLI** - Rich UI with auto-discovery and guided workflows
- âš¡ **GPU Acceleration** - NVIDIA CUDA support for 5-10x faster inference
- ğŸ¤– **Multi-Model Support** - Ollama, Hugging Face, OpenAI integration
- ğŸ“Š **Comprehensive Analysis** - Statistical bias detection with visualizations
- ğŸ³ **Docker Integration** - Containerized deployment with GPU passthrough
- ğŸ“ **Session Management** - Organized output with resumable workflows
- ğŸ”§ **Professional UX** - Unicode-safe, cross-platform compatibility

## ğŸš€ Quick Start

### âœ… Setup Verification

```bash
# ğŸ” Verify your setup (recommended first step)
python verify_setup.py

# The verification script checks:
# - Python version compatibility
# - Required packages installation
# - Directory structure integrity
# - Docker availability
# - Ollama connection
# - GPU support detection
# - System resources
```

### ğŸ¯ One-Command Experience (Recommended)

```bash
# ğŸš€ Launch Interactive Interface
uv run equilens --help

# âœ¨ The CLI will:
# - Auto-discover corpus files and available models
# - Present beautiful selection panels with Rich UI
# - Guide you through the complete bias detection workflow
# - Save all results in organized session directories
```

### ğŸ”§ Manual Setup

```bash
# 1. ğŸ“¦ Environment Setup
uv venv
uv pip install -r pyproject.toml

# 2. ğŸ³ Start Services (Docker)
docker compose up -d

# 3. ğŸ” Run Bias Audit
uv run python src/Phase2_ModelAuditor/audit_model.py \
  --model phi3:mini \
  --corpus src/Phase1_CorpusGenerator/corpus/audit_corpus_gender_bias.csv

# 4. ğŸ“Š Analyze Results
uv run python src/Phase3_Analysis/analyze_results.py \
  --results_file results/results_phi3_mini_*.csv
```

### 4. Platform Launchers (Auto-activates venv)
```bash
# Windows (double-click or command line)
equilens.bat gpu-check
equilens.bat start

# Linux/macOS
chmod +x equilens.sh
./equilens.sh gpu-check
./equilens.sh start
```

## ğŸ® GPU Acceleration

EquiLens automatically detects and uses GPU acceleration when available:

- **ğŸ” Check GPU Status**: `python equilens.py gpu-check`
- **ğŸ¯ Auto-Detection**: GPU automatically used if CUDA + Docker GPU support available
- **âš¡ CPU Fallback**: Seamless fallback to CPU-only mode
- **ğŸ“‹ Setup Guidance**: Direct links to NVIDIA CUDA downloads

**Performance Impact**: 5-10x faster model inference with GPU acceleration

```
./equilens.sh start
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Host Machine  â”‚â”€â”€â”€â–¶â”‚   Docker Engine  â”‚â”€â”€â”€â–¶â”‚   Containers    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ equilens.py     â”‚    â”‚ docker-compose   â”‚    â”‚ â€¢ Ollama        â”‚
â”‚ equilens.bat    â”‚    â”‚                  â”‚    â”‚ â€¢ EquiLens App  â”‚
â”‚ equilens.sh     â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Available Commands

| Command | Description | Example |
|---------|-------------|---------|
| `start` | Start all services | `python equilens.py start` |
| `stop` | Stop all services | `python equilens.py stop` |
| `status` | Show service status | `python equilens.py status` |
| `models list` | List available models | `python equilens.py models list` |
| `models pull <name>` | Download model | `python equilens.py models pull llama3.2` |
| `audit <config>` | Run bias audit | `python equilens.py audit config.json` |
| `generate <config>` | Generate test corpus | `python equilens.py generate config.json` |
| `analyze <results>` | Analyze results | `python equilens.py analyze results.csv` |

## ğŸ¯ Key Features

### âœ… **Smart Ollama Detection**
- Automatically detects existing Ollama containers
- Uses external Ollama if available and accessible
- Creates new Ollama container only if needed
- Preserves model downloads across restarts

### âœ… **Platform Independence**
- Single Python CLI works on Windows, Linux, macOS
- Optional platform launchers for convenience
- No platform-specific dependencies
- Consistent experience across environments

### âœ… **Persistent Model Storage**
- Models stored in Docker volumes
- Survive container restarts
- No re-downloading after `docker compose down`
- Efficient model sharing between runs

### âœ… **GPU Acceleration**
- Automatic NVIDIA GPU detection and utilization
- Fallback to CPU if GPU unavailable
- Optimized for Windows 11 + RTX GPUs

### âœ… **Fast Dependency Management**
- Uses `uv` for lightning-fast package installation
- Virtual environment isolation
- Automatic dependency resolution
- No conflicts with system Python

## ğŸ› ï¸ Development Tools

```bash
# Create new bias configuration
python tools/quick_setup.py

# Validate configuration
python tools/validate_config.py config.json

# Run mock Ollama for testing
python tools/mock_ollama.py
```

## ğŸ“ Project Structure

```
EquiLens/
â”œâ”€â”€ equilens.py              # ğŸ¯ Main unified CLI
â”œâ”€â”€ equilens.bat             # ğŸªŸ Windows launcher
â”œâ”€â”€ equilens.sh              # ğŸ§ Unix/Linux launcher
â”œâ”€â”€ docker-compose.yml       # ğŸ³ Container orchestration
â”œâ”€â”€ Dockerfile               # ğŸ“¦ App container definition
â”œâ”€â”€ requirements.txt         # ğŸ“‹ Python dependencies
â”œâ”€â”€ Phase1_CorpusGenerator/  # ğŸ“ Corpus generation
â”œâ”€â”€ Phase2_ModelAuditor/     # ğŸ” Bias auditing
â”œâ”€â”€ Phase3_Analysis/         # ğŸ“Š Result analysis
â”œâ”€â”€ results/                 # ğŸ“ˆ Audit outputs
â”œâ”€â”€ docs/                    # ğŸ“š Documentation
â””â”€â”€ tools/                   # ğŸ› ï¸ Development utilities
```

## ğŸ”§ Configuration

### Example Bias Configuration
```json
{
  "bias_type": "gender",
  "target_words": ["doctor", "nurse", "engineer"],
  "bias_words": {
    "male": ["he", "him", "man"],
    "female": ["she", "her", "woman"]
  },
  "templates": [
    "The {target} said {pronoun} would help.",
    "{pronoun} is a skilled {target}."
  ]
}
```

## ğŸ“Š Example Workflow

```bash
# 1. Start services
python equilens.py start

# 2. Download a model
python equilens.py models pull phi3:mini

# 3. Generate test corpus
python equilens.py generate bias_config.json

# 4. Run bias audit
python equilens.py audit bias_config.json
```

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "ğŸ¯ EquiLens Platform"
        CLI[ğŸ–¥ï¸ Interactive CLI<br/>Rich UI & Auto-discovery]
        TUI[ğŸ“Ÿ Terminal UI<br/>Progress Monitoring]
        WEB[ğŸŒ Web Interface<br/>Future Dashboard]

        CLI --> MANAGER[ğŸ›ï¸ Core Manager]
        TUI --> MANAGER
        WEB --> MANAGER

        MANAGER --> PHASE2[ğŸ” Model Auditor]
        MANAGER --> PHASE1[ğŸ“ Corpus Generator]
        MANAGER --> PHASE3[ğŸ“Š Results Analyzer]

        PHASE2 --> OLLAMA[ğŸ¤– Ollama Service]
        OLLAMA --> GPU[âš¡ GPU Acceleration]

        PHASE1 --> STORAGE[ğŸ“ Session Storage]
        PHASE2 --> STORAGE
        PHASE3 --> STORAGE
    end
```

## ğŸ“Š Results & Output

After running EquiLens, your results are organized in session directories:

```
results/
â””â”€â”€ ğŸ“ phi3_mini_20250808_123456/
    â”œâ”€â”€ ğŸ“Š results_phi3_mini_20250808_123456.csv   # Detailed audit data
    â”œâ”€â”€ ğŸ“‹ progress_20250808_123456.json           # Session progress
    â”œâ”€â”€ ğŸ“ summary_20250808_123456.json            # Session summary
    â”œâ”€â”€ ğŸ“ˆ bias_report.png                         # Bias visualization
    â”œâ”€â”€ ğŸ“‹ session_metadata.json                   # Configuration
    â””â”€â”€ ğŸ“œ session.log                             # Execution log
```

### ğŸ“ˆ Sample Bias Report

The bias analysis includes:
- Statistical significance testing
- Bias score calculations
- Visual bias distribution charts
- Detailed recommendations for model improvement

## ğŸ”§ Advanced Configuration

### ğŸ“‹ Custom Corpus Generation

```json
{
  "bias_categories": {
    "gender": {
      "male_words": ["he", "him", "his", "man", "boy"],
      "female_words": ["she", "her", "hers", "woman", "girl"],
      "neutral_words": ["person", "individual", "someone"]
    }
  },
  "prompt_templates": [
    "The {category} is good at {skill}",
    "{category} people are known for {trait}"
  ]
}
```

### âš™ï¸ Model Configuration

```json
{
  "model_settings": {
    "temperature": 0.7,
    "max_tokens": 100,
    "timeout": 30
  },
  "audit_settings": {
    "batch_size": 10,
    "retry_attempts": 3,
    "progress_checkpoint": 10
  }
}
```

## ğŸ® GPU Acceleration

EquiLens automatically detects and utilizes GPU acceleration:

```bash
# Check GPU availability
nvidia-smi

# Verify GPU usage in EquiLens
uv run equilens status
# The CLI will show GPU status during model detection
```

**Performance Benefits:**
- ğŸš€ **5-10x faster** model inference with GPU
- âš¡ Automatic GPU detection and configuration
- ğŸ”„ Graceful fallback to CPU-only mode
- ğŸ“Š Real-time performance monitoring

## ğŸ“š Documentation

- **ğŸ“– [QUICKSTART.md](docs/QUICKSTART.md)** - Quick setup guide
- **ğŸ“– [PIPELINE.md](docs/PIPELINE.md)** - Complete workflow guide
- **ğŸ“– [ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System architecture details
- **ğŸ“– [CONFIGURATION_GUIDE.md](docs/CONFIGURATION_GUIDE.md)** - Advanced configuration
- **ğŸ“– [EXECUTION_GUIDE.md](docs/EXECUTION_GUIDE.md)** - Detailed execution instructions
- **ğŸ“– [OLLAMA_SETUP.md](docs/OLLAMA_SETUP.md)** - Ollama configuration guide

## ğŸ› Troubleshooting

### Quick Diagnostics
```bash
# ğŸ” Comprehensive system check
uv run equilens --help

# ğŸ³ Docker status
docker compose ps

# ğŸ¤– Ollama connectivity
curl http://localhost:11434/api/tags
```

### Common Solutions
| Issue                | Symptoms               | Solution                            |
| -------------------- | ---------------------- | ----------------------------------- |
| **Unicode Errors**   | Emoji display issues   | Handled automatically by CLI        |
| **Model Not Found**  | Auto-discovery fails   | Check Ollama service status         |
| **GPU Not Detected** | Slow inference         | Verify NVIDIA drivers & Docker GPU  |
| **File Permissions** | Session creation fails | Check write permissions in results/ |

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’» Make your changes
4. âœ… Test with the interactive CLI
5. ğŸ“ Commit your changes (`git commit -m 'Add amazing feature'`)
6. ğŸš€ Push to the branch (`git push origin feature/amazing-feature`)
7. ğŸ”„ Open a Pull Request

## Requirements

- **Python 3.13+** for latest features and performance
- **Docker Desktop** with Compose V2 support
- **uv** package manager (recommended over pip)
- **NVIDIA GPU** (optional, for acceleration)
- **4GB+ RAM** recommended for model processing

## Compatibility

- âœ… **Windows 10/11** (WSL2 recommended for Docker)
- âœ… **macOS** (Intel and Apple Silicon)
- âœ… **Linux** (Ubuntu 20.04+, Fedora, Arch)
- âœ… **Docker Desktop** or **Docker Engine**
- âœ… **VS Code** with Dev Containers extension

## Docker Setup

### Quick Start
```bash
# ğŸš€ One-command setup
docker compose up -d

# ğŸ” Verify services
docker compose ps
```

### GPU Configuration
```bash
# ğŸ® GPU-enabled setup
docker compose -f docker-compose.gpu.yml up -d

# âœ… Test GPU access
docker exec -it equilens-ollama-1 nvidia-smi
```

## License

This project is licensed under the **Apache License 2.0** - see the [LICENSE.md](LICENSE.md) file for details.

### ğŸ¯ Quick License Summary
- âœ… **Commercial use allowed**
- âœ… **Modification and distribution permitted**
- âœ… **Patent protection included**
- ğŸ“‹ **Attribution required**
- ğŸ›¡ï¸ **No warranty provided**

## ğŸ‰ Success Story

EquiLens has evolved from a complex multi-script system to a streamlined, production-ready platform:

- ğŸ¨ **Enhanced User Experience**: Interactive CLI with Rich UI
- ğŸ”§ **Simplified Workflow**: Auto-discovery and guided setup
- âš¡ **Performance Optimized**: GPU acceleration and efficient processing
- ğŸ“ **Organized Output**: Session-based file management
- ğŸ³ **Container Ready**: Docker integration with GPU support
- ğŸ“Š **Professional Results**: Comprehensive bias analysis and reporting

**Ready to detect AI bias?** Start with `uv run equilens --help` and experience the difference! ğŸš€

---

>**ğŸ’¡ Pro Tip**: Start with `uv run equilens status` to check your system, then `uv run equilens start` to begin!
