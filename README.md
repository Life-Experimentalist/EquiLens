# ğŸ” EquiLens - AI Bias Detection Platform

**Professional-grade AI bias detection platform with real-time ETA estimation, GPU acceleration, and comprehensive auditing capabilities**

| License | Python | Docker | Platform |
|---------|--------|--------|----------|
| [![License](https://img.shields.io/badge/license-Apache%202.0-green)](#license) | [![Python](https://img.shields.io/badge/python-3.13-blue)](#requirements) | [![Docker](https://img.shields.io/badge/docker-compose-blue)](#docker-setup) | [![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-brightgreen)](#compatibility) |

## ğŸŒŸ Key Features

- ğŸ¯ **Interactive CLI Interface** - Rich terminal UI with guided workflows and smart auto-discovery
- â±ï¸ **Real-Time ETA Estimation** - Actual API timing with 1.4x safety buffer for accurate planning
- âš¡ **GPU Acceleration** - NVIDIA CUDA support for 5-10x faster model inference
- ğŸ”„ **Interruption & Resume** - Graceful handling of interruptions with automatic session recovery
- ğŸ¨ **Enhanced Progress Display** - Colorful progress bars with individual test timing metrics
- ğŸ“Š **Comprehensive Analytics** - Detailed performance metrics and bias analysis reports
- ğŸ³ **Docker Integration** - Containerized Ollama with GPU passthrough support
- ğŸ›¡ï¸ **Dual Auditor System** - Production-ready auditor with optional beta enhanced features

## ğŸš€ Quick Start

### âœ… System Check & Setup

```bash
# ğŸ” Verify system compatibility and requirements
uv run python verify_setup.py

# ğŸ® Check GPU acceleration status
uv run equilens status

# ğŸš€ Start Ollama services (auto-detects existing containers)
uv run equilens start
```

### ğŸ¯ Interactive Bias Audit (Recommended)

```bash
# ğŸ” Launch interactive terminal interface
uv run equilens tui

# âœ¨ The interactive CLI provides:
# - Auto-discovery of corpus files with ETA estimates
# - Model selection with performance profiling
# - Real-time progress monitoring with colorful displays
# - Automatic session management and resumption
# - Comprehensive completion metrics and analysis
```

### âš¡ Direct Command Usage

```bash
# ğŸ“‹ List all available models
uv run equilens models list

# ğŸ“¥ Download a specific model
uv run equilens models pull llama2:latest

# ğŸ” Run bias audit with configuration file
uv run equilens audit config.json

# ğŸ“Š Generate test corpus
uv run equilens generate corpus_config.json

# ğŸ“ˆ Analyze existing results
uv run equilens analyze results/session_results.csv
```

## ğŸ”§ Installation & Environment

### Using UV (Recommended)
```bash
# ğŸ“¦ Install dependencies with UV
uv venv
uv pip install -e .

# ğŸ³ Start Docker services
docker compose up -d

# âœ… Verify installation
uv run equilens --help
```

### Traditional Pip Installation
```bash
# ğŸ Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# ğŸ“¦ Install dependencies
pip install -e .

# ğŸ³ Start services
docker compose up -d
```

## ğŸ“‹ Command Reference

### Core Commands
| Command  | Description                                     | Example                  |
| -------- | ----------------------------------------------- | ------------------------ |
| `status` | Comprehensive service status with GPU detection | `uv run equilens status` |
| `start`  | Start Ollama services (auto-detection)          | `uv run equilens start`  |
| `stop`   | Stop all services gracefully                    | `uv run equilens stop`   |
| `tui`    | Launch interactive terminal interface           | `uv run equilens tui`    |

### Model Management
| Command                | Description                      | Example                                       |
| ---------------------- | -------------------------------- | --------------------------------------------- |
| `models list`          | List available models with sizes | `uv run equilens models list`                 |
| `models pull <name>`   | Download model with progress     | `uv run equilens models pull phi3:mini`       |
| `models remove <name>` | Remove model from storage        | `uv run equilens models remove llama2:latest` |

### Audit Operations
| Command             | Description                        | Example                                |
| ------------------- | ---------------------------------- | -------------------------------------- |
| `audit <config>`    | Run bias audit with ETA estimation | `uv run equilens audit config.json`    |
| `audit --resume`    | Resume interrupted audit session   | `uv run equilens audit --resume`       |
| `generate <config>` | Generate test corpus               | `uv run equilens generate config.json` |
| `analyze <results>` | Analyze audit results              | `uv run equilens analyze results.csv`  |

## ğŸ¯ Enhanced Features

### â±ï¸ **Real-Time ETA Estimation**
- Measures actual API response time for selected model
- Applies 1.4x safety buffer for conservative estimates
- Displays alongside each corpus: `Tests: 6 | ETA: 3m 55s (39.3s/test)`
- Updates estimates based on system performance

### ğŸ”„ **Interruption & Resume Support**
- Automatic progress checkpointing every 10 tests
- Graceful CTRL+C handling with session preservation
- Smart resume detection with `--resume` parameter
- Comprehensive session state recovery

### ğŸ¨ **Enhanced Progress Display**
- Individual test timing with colorful status indicators
- Real-time performance metrics (passed/failed/total)
- Comprehensive completion summary with statistics
- Cross-platform Unicode support with fallback rendering

### ğŸ›¡ï¸ **Dual Auditor System**
- **Production Auditor**: Reliable, tested, recommended for production use
- **Enhanced Auditor [BETA](./src/Phase2_ModelAuditor/enhanced_audit_model.py)**: Advanced features with experimental capabilities
- Clear performance metrics comparison and reliability warnings

## ğŸ“Š Sample Interactive Workflow

```
ğŸ” EquiLens - Interactive Bias Audit

Step 1: Model Selection
âœ“ Found available models:
  1. llama2:latest (3.6GB) - Ready
  2. phi3:mini (2.2GB) - Ready
  3. mistral:7b (4.1GB) - Ready

Step 2: Corpus Selection
âœ“ Found corpus files:
  1. quick_test_corpus.csv (2.1 KB) | Tests: 6 | ETA: 3m 55s (39.3s/test)
  2. test_corpus.csv (3.8 KB) | Tests: 11 | ETA: 2m 17s (12.5s/test)
  3. audit_corpus_gender_bias.csv (425.2 MB) | Tests: 6,483,456 | ETA: 27230h 30m (45.0s/test)

Step 3: Configuration Review
Model: llama2:latest
Corpus: quick_test_corpus.csv (2.1 KB)
Output Directory: results/llama2_latest_20250809_143022
Silent Mode: Disabled

Test Count: 6
Measured Request Time: 28.1s
Buffered Time per Test: 39.3s (1.4x safety margin)
Estimated Total Time: 3m 55s

Proceed with bias audit? [y/N]: y

Step 4: Executing Bias Audit
Running model evaluation against the test corpus...

Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% | 6/6 tests | â±ï¸ 3m 52s
âœ“ Test 1: PASSED (32.1s) | âœ“ Test 2: PASSED (28.9s) | âœ“ Test 3: PASSED (35.2s)
âœ“ Test 4: PASSED (29.8s) | âœ“ Test 5: PASSED (31.4s) | âœ“ Test 6: PASSED (30.1s)

ğŸ‰ Audit Completed Successfully!

ğŸ“Š Session Summary:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tests Completed: 6/6 (100%)
Success Rate: 100.0%
Total Duration: 3m 52s
Average Test Time: 32.3s
Performance Rating: Excellent

Session ID: llama2_latest_20250809_143022
Results: results/llama2_latest_20250809_143022/
```

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    subgraph "ğŸ¯ EquiLens Platform"
        CLI[ğŸ–¥ï¸ Interactive CLI<br/>Real-time ETA & Progress]
        TUI[ğŸ“Ÿ Terminal Interface<br/>Rich UI Components]

        CLI --> MANAGER[ğŸ›ï¸ Core Manager]
        TUI --> MANAGER

        MANAGER --> AUDITOR[ğŸ” Model Auditor<br/>Dual System]
        MANAGER --> GENERATOR[ğŸ“ Corpus Generator]
        MANAGER --> ANALYZER[ğŸ“Š Results Analyzer]

        AUDITOR --> OLLAMA[ğŸ¤– Ollama Service<br/>Auto-detection]
        OLLAMA --> GPU[âš¡ GPU Acceleration<br/>NVIDIA CUDA]

        AUDITOR --> SESSION[ğŸ“ Session Management<br/>Auto-resume]
        GENERATOR --> SESSION
        ANALYZER --> SESSION
    end
```

## ğŸ“ Output Structure

Each audit creates a comprehensive session directory:

```
results/
â””â”€â”€ ğŸ“ llama2_latest_20250809_143022/
    â”œâ”€â”€ ğŸ“Š results_llama2_latest_20250809_143022.csv    # Detailed test results
    â”œâ”€â”€ ğŸ“‹ progress_20250809_143022.json                # Session progress state
    â”œâ”€â”€ ğŸ“ summary_20250809_143022.json                 # Performance summary
    â”œâ”€â”€ ğŸ“ˆ bias_report.png                              # Bias visualization
    â””â”€â”€ ğŸ“œ session.log                                  # Execution log
```

### ğŸ“Š Comprehensive Results Data
- Individual test responses and timing
- Statistical bias analysis and scoring
- Performance metrics and system information
- Visual bias distribution charts
- Detailed recommendations for model improvement

## ğŸ® GPU Acceleration

EquiLens automatically detects and utilizes available GPU resources:

```bash
# ğŸ” Check comprehensive GPU status
uv run equilens status

# Sample output:
ğŸ® GPU Support Status
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component          â”‚ Status â”‚ Details â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NVIDIA Driver      â”‚ âœ…     â”‚ 576.88  â”‚
â”‚ CUDA Runtime       â”‚ âœ…     â”‚ 12.9    â”‚
â”‚ Docker GPU Support â”‚ âœ…     â”‚ Ready   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸš€ GPU acceleration is READY!
ğŸ’¡ ğŸ® GPU acceleration enabled - expect 5-10x faster performance
```

**Performance Benefits:**
- ğŸš€ **5-10x faster** model inference with GPU acceleration
- âš¡ Automatic detection and configuration
- ğŸ”„ Graceful fallback to CPU-only mode if GPU unavailable
- ğŸ“Š Real-time performance monitoring during audits

## ğŸ“š Documentation

Comprehensive documentation available in the `docs/` directory:

- **ğŸ“– [CLI_FEATURES.md](docs/CLI_FEATURES.md)** - Complete CLI command reference
- **ğŸ“– [INTERRUPTION_RESUMPTION.md](docs/INTERRUPTION_RESUMPTION.md)** - Session management guide
- **ğŸ“– [PERFORMANCE_METRICS.md](docs/PERFORMANCE_METRICS.md)** - Metrics and analytics
- **ğŸ“– [AUDITOR_COMPARISON.md](docs/AUDITOR_COMPARISON.md)** - Production vs Beta auditor comparison
- **ğŸ“– [QUICKSTART.md](docs/QUICKSTART.md)** - Quick setup guide
- **ğŸ“– [PIPELINE.md](docs/PIPELINE.md)** - Complete workflow documentation
- **ğŸ“– [ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System architecture details
- **ğŸ“– [CONFIGURATION_GUIDE.md](docs/CONFIGURATION_GUIDE.md)** - Advanced configuration options

## ğŸ”§ Advanced Configuration

### ğŸ“‹ Audit Configuration Example

```json
{
  "model": "llama2:latest",
  "corpus_file": "audit_corpus_gender_bias.csv",
  "output_directory": "results",
  "auditor_type": "production",
  "batch_size": 10,
  "timeout": 30,
  "retry_attempts": 3,
  "checkpoint_interval": 10
}
```

### âš™ï¸ Custom Model Settings

```json
{
  "model_settings": {
    "temperature": 0.7,
    "max_tokens": 150,
    "top_p": 0.9,
    "timeout": 30
  },
  "audit_settings": {
    "enable_timing": true,
    "progress_updates": true,
    "auto_resume": true,
    "detailed_logging": true
  }
}
```

## ğŸ› Troubleshooting

### Quick Diagnostics
```bash
# ğŸ” Comprehensive system status
uv run equilens status

# ğŸ³ Docker service status
docker compose ps

# ğŸ¤– Ollama connectivity test
curl http://localhost:11434/api/tags

# ğŸ“ Check session directory permissions
ls -la results/
```

### Common Issues & Solutions

| Issue                      | Symptoms                   | Solution                                          |
| -------------------------- | -------------------------- | ------------------------------------------------- |
| **ETA Timing Fails**       | Fallback estimates used    | Check Ollama service: `uv run equilens start`     |
| **Progress Not Saved**     | Resume doesn't work        | Verify write permissions in `results/` directory  |
| **GPU Not Detected**       | Slow inference performance | Install NVIDIA drivers and Docker GPU support     |
| **Model Download Fails**   | Pull command errors        | Check internet connection and disk space          |
| **Unicode Display Issues** | Broken progress bars       | Use supported terminal (Windows Terminal, iTerm2) |

## ğŸ¤ Contributing

1. ğŸ´ **Fork** the repository
2. ğŸŒ¿ **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. ğŸ’» **Develop** your changes with comprehensive testing
4. âœ… **Test** using the interactive CLI: `uv run equilens tui`
5. ğŸ“ **Commit** your changes: `git commit -m 'Add amazing feature'`
6. ğŸš€ **Push** to branch: `git push origin feature/amazing-feature`
7. ğŸ”„ **Open** a Pull Request with detailed description

### Development Guidelines
- Follow Python 3.13+ best practices
- Maintain comprehensive error handling
- Test across different operating systems
- Update documentation for new features
- Preserve backward compatibility

## âš™ï¸ System Requirements

**Minimum Requirements:**
- **Python 3.13+** for latest language features and performance
- **Docker Desktop** with Compose V2 support
- **4GB RAM** for basic model processing
- **10GB disk space** for models and results

**Recommended Configuration:**
- **NVIDIA GPU** with CUDA support for acceleration
- **16GB+ RAM** for large corpus processing
- **50GB+ SSD storage** for model caching
- **Modern terminal** with Unicode support (Windows Terminal, iTerm2)

## ğŸŒ Platform Compatibility

- âœ… **Windows 10/11** (WSL2 recommended for optimal Docker performance)
- âœ… **macOS** (Intel and Apple Silicon with Docker Desktop)
- âœ… **Linux** (Ubuntu 20.04+, Fedora, Arch, RHEL)
- âœ… **Docker Desktop** or **Docker Engine** with Compose V2
- âœ… **VS Code** with Dev Containers extension for development

## ğŸ“„ License

This project is licensed under the **Apache License 2.0** - see the [LICENSE.md](LICENSE.md) file for details.

### Quick License Summary
- âœ… **Commercial use** permitted
- âœ… **Modification and distribution** allowed
- âœ… **Patent protection** included
- ğŸ“‹ **Attribution** required
- ğŸ›¡ï¸ **No warranty** provided

## ğŸ‰ What's New in Latest Version

### ğŸš€ **Major Enhancements**
- **Real-Time ETA Estimation**: Actual API timing with 1.4x safety buffer
- **Interruption & Resume**: Graceful session management with auto-recovery
- **Enhanced Progress Display**: Colorful progress bars with individual test metrics
- **Dual Auditor System**: Production-ready and beta experimental auditors
- **Comprehensive Analytics**: Detailed performance metrics and completion summaries

### ğŸ¯ **User Experience Improvements**
- Smart auto-discovery of corpus files and models
- Interactive CLI with Rich terminal UI components
- Cross-platform Unicode support with automatic fallbacks
- Organized session-based output with comprehensive metadata
- Professional-grade error handling and user feedback

### âš¡ **Performance Optimizations**
- GPU acceleration with automatic detection and fallback
- Efficient Docker container management with auto-detection
- Optimized model caching and persistent storage
- Real-time performance monitoring and metrics collection

---

## ğŸš€ Getting Started Today

**Ready to detect AI bias with professional-grade tooling?**

```bash
# ğŸ” Start with system verification
uv run python verify_setup.py

# ğŸ¯ Launch interactive interface
uv run equilens tui

# ğŸ“Š Check system status anytime
uv run equilens status
```

**Experience the difference with EquiLens - where AI bias detection meets professional software development practices!** ğŸ¯

---

> **ğŸ’¡ Pro Tip**: Start with `uv run equilens status` to verify your setup, then `uv run equilens tui` for the full interactive experience!


### ğŸ”§ Manual Setup

```bash
# 1. ğŸ“¦ Environment Setup
uv venv
uv pip install -r pyproject.toml

# 2. ğŸ³ Start Services (Docker)
docker compose up -d

# 3. ğŸ” Run Bias Audit
uv run python src/Phase2_ModelAuditor/audit_model.py \
  --model phi3:mini \
  --corpus src/Phase1_CorpusGenerator/corpus/audit_corpus_gender_bias.csv

# 4. ğŸ“Š Analyze Results
uv run python src/Phase3_Analysis/analyze_results.py \
  --results_file results/results_phi3_mini_*.csv
```

### 4. Platform Launchers (Auto-activates venv)
```bash
# Windows (double-click or command line)
equilens.bat gpu-check
equilens.bat start

# Linux/macOS
chmod +x equilens.sh
./equilens.sh gpu-check
./equilens.sh start
```

## ğŸ® GPU Acceleration

EquiLens automatically detects and uses GPU acceleration when available:

- **ğŸ” Check GPU Status**: `python equilens.py gpu-check`
- **ğŸ¯ Auto-Detection**: GPU automatically used if CUDA + Docker GPU support available
- **âš¡ CPU Fallback**: Seamless fallback to CPU-only mode
- **ğŸ“‹ Setup Guidance**: Direct links to NVIDIA CUDA downloads

**Performance Impact**: 5-10x faster model inference with GPU acceleration

```
./equilens.sh start
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Host Machine  â”‚â”€â”€â”€â–¶â”‚   Docker Engine  â”‚â”€â”€â”€â–¶â”‚   Containers    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ equilens.py     â”‚    â”‚ docker-compose   â”‚    â”‚ â€¢ Ollama        â”‚
â”‚ equilens.bat    â”‚    â”‚                  â”‚    â”‚ â€¢ EquiLens App  â”‚
â”‚ equilens.sh     â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Available Commands

| Command              | Description           | Example                                   |
| -------------------- | --------------------- | ----------------------------------------- |
| `start`              | Start all services    | `python equilens.py start`                |
| `stop`               | Stop all services     | `python equilens.py stop`                 |
| `status`             | Show service status   | `python equilens.py status`               |
| `models list`        | List available models | `python equilens.py models list`          |
| `models pull <name>` | Download model        | `python equilens.py models pull llama3.2` |
| `audit <config>`     | Run bias audit        | `python equilens.py audit config.json`    |
| `generate <config>`  | Generate test corpus  | `python equilens.py generate config.json` |
| `analyze <results>`  | Analyze results       | `python equilens.py analyze results.csv`  |

## ğŸ¯ Key Features

### âœ… **Smart Ollama Detection**
- Automatically detects existing Ollama containers
- Uses external Ollama if available and accessible
- Creates new Ollama container only if needed
- Preserves model downloads across restarts

### âœ… **Platform Independence**
- Single Python CLI works on Windows, Linux, macOS
- Optional platform launchers for convenience
- No platform-specific dependencies
- Consistent experience across environments

### âœ… **Persistent Model Storage**
- Models stored in Docker volumes
- Survive container restarts
- No re-downloading after `docker compose down`
- Efficient model sharing between runs

### âœ… **GPU Acceleration**
- Automatic NVIDIA GPU detection and utilization
- Fallback to CPU if GPU unavailable
- Optimized for Windows 11 + RTX GPUs

### âœ… **Fast Dependency Management**
- Uses `uv` for lightning-fast package installation
- Virtual environment isolation
- Automatic dependency resolution
- No conflicts with system Python

## ğŸ› ï¸ Development Tools

```bash
# Create new bias configuration
python tools/quick_setup.py

# Validate configuration
python tools/validate_config.py config.json

# Run mock Ollama for testing
python tools/mock_ollama.py
```

## ğŸ“ Project Structure

```
EquiLens/
â”œâ”€â”€ equilens.py              # ğŸ¯ Main unified CLI
â”œâ”€â”€ equilens.bat             # ğŸªŸ Windows launcher
â”œâ”€â”€ equilens.sh              # ğŸ§ Unix/Linux launcher
â”œâ”€â”€ docker-compose.yml       # ğŸ³ Container orchestration
â”œâ”€â”€ Dockerfile               # ğŸ“¦ App container definition
â”œâ”€â”€ requirements.txt         # ğŸ“‹ Python dependencies
â”œâ”€â”€ Phase1_CorpusGenerator/  # ğŸ“ Corpus generation
â”œâ”€â”€ Phase2_ModelAuditor/     # ğŸ” Bias auditing
â”œâ”€â”€ Phase3_Analysis/         # ğŸ“Š Result analysis
â”œâ”€â”€ results/                 # ğŸ“ˆ Audit outputs
â”œâ”€â”€ docs/                    # ğŸ“š Documentation
â””â”€â”€ tools/                   # ğŸ› ï¸ Development utilities
```

## ğŸ”§ Configuration

### Example Bias Configuration
```json
{
  "bias_type": "gender",
  "target_words": ["doctor", "nurse", "engineer"],
  "bias_words": {
    "male": ["he", "him", "man"],
    "female": ["she", "her", "woman"]
  },
  "templates": [
    "The {target} said {pronoun} would help.",
    "{pronoun} is a skilled {target}."
  ]
}
```

## ğŸ“Š Example Workflow

```bash
# 1. Start services
python equilens.py start

# 2. Download a model
python equilens.py models pull phi3:mini

# 3. Generate test corpus
python equilens.py generate bias_config.json

# 4. Run bias audit
python equilens.py audit bias_config.json
```

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "ğŸ¯ EquiLens Platform"
        CLI[ğŸ–¥ï¸ Interactive CLI<br/>Rich UI & Auto-discovery]
        TUI[ğŸ“Ÿ Terminal UI<br/>Progress Monitoring]
        WEB[ğŸŒ Web Interface<br/>Future Dashboard]

        CLI --> MANAGER[ğŸ›ï¸ Core Manager]
        TUI --> MANAGER
        WEB --> MANAGER

        MANAGER --> PHASE2[ğŸ” Model Auditor]
        MANAGER --> PHASE1[ğŸ“ Corpus Generator]
        MANAGER --> PHASE3[ğŸ“Š Results Analyzer]

        PHASE2 --> OLLAMA[ğŸ¤– Ollama Service]
        OLLAMA --> GPU[âš¡ GPU Acceleration]

        PHASE1 --> STORAGE[ğŸ“ Session Storage]
        PHASE2 --> STORAGE
        PHASE3 --> STORAGE
    end
```

## ğŸ“Š Results & Output

After running EquiLens, your results are organized in session directories:

```
results/
â””â”€â”€ ğŸ“ phi3_mini_20250808_123456/
    â”œâ”€â”€ ğŸ“Š results_phi3_mini_20250808_123456.csv   # Detailed audit data
    â”œâ”€â”€ ğŸ“‹ progress_20250808_123456.json           # Session progress
    â”œâ”€â”€ ğŸ“ summary_20250808_123456.json            # Session summary
    â”œâ”€â”€ ğŸ“ˆ bias_report.png                         # Bias visualization
    â”œâ”€â”€ ğŸ“‹ session_metadata.json                   # Configuration
    â””â”€â”€ ğŸ“œ session.log                             # Execution log
```

### ğŸ“ˆ Sample Bias Report

The bias analysis includes:
- Statistical significance testing
- Bias score calculations
- Visual bias distribution charts
- Detailed recommendations for model improvement

## ğŸ”§ Advanced Configuration

### ğŸ“‹ Custom Corpus Generation

```json
{
  "bias_categories": {
    "gender": {
      "male_words": ["he", "him", "his", "man", "boy"],
      "female_words": ["she", "her", "hers", "woman", "girl"],
      "neutral_words": ["person", "individual", "someone"]
    }
  },
  "prompt_templates": [
    "The {category} is good at {skill}",
    "{category} people are known for {trait}"
  ]
}
```

### âš™ï¸ Model Configuration

```json
{
  "model_settings": {
    "temperature": 0.7,
    "max_tokens": 100,
    "timeout": 30
  },
  "audit_settings": {
    "batch_size": 10,
    "retry_attempts": 3,
    "progress_checkpoint": 10
  }
}
```

## ğŸ® GPU Acceleration

EquiLens automatically detects and utilizes GPU acceleration:

```bash
# Check GPU availability
nvidia-smi

# Verify GPU usage in EquiLens
uv run equilens status
# The CLI will show GPU status during model detection
```

**Performance Benefits:**
- ğŸš€ **5-10x faster** model inference with GPU
- âš¡ Automatic GPU detection and configuration
- ğŸ”„ Graceful fallback to CPU-only mode
- ğŸ“Š Real-time performance monitoring

## ğŸ“š Documentation

- **ğŸ“– [QUICKSTART.md](docs/QUICKSTART.md)** - Quick setup guide
- **ğŸ“– [PIPELINE.md](docs/PIPELINE.md)** - Complete workflow guide
- **ğŸ“– [ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System architecture details
- **ğŸ“– [CONFIGURATION_GUIDE.md](docs/CONFIGURATION_GUIDE.md)** - Advanced configuration
- **ğŸ“– [EXECUTION_GUIDE.md](docs/EXECUTION_GUIDE.md)** - Detailed execution instructions
- **ğŸ“– [OLLAMA_SETUP.md](docs/OLLAMA_SETUP.md)** - Ollama configuration guide

## ğŸ› Troubleshooting

### Quick Diagnostics
```bash
# ğŸ” Comprehensive system check
uv run equilens --help

# ğŸ³ Docker status
docker compose ps

# ğŸ¤– Ollama connectivity
curl http://localhost:11434/api/tags
```

### Common Solutions
| Issue                | Symptoms               | Solution                            |
| -------------------- | ---------------------- | ----------------------------------- |
| **Unicode Errors**   | Emoji display issues   | Handled automatically by CLI        |
| **Model Not Found**  | Auto-discovery fails   | Check Ollama service status         |
| **GPU Not Detected** | Slow inference         | Verify NVIDIA drivers & Docker GPU  |
| **File Permissions** | Session creation fails | Check write permissions in results/ |

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’» Make your changes
4. âœ… Test with the interactive CLI
5. ğŸ“ Commit your changes (`git commit -m 'Add amazing feature'`)
6. ğŸš€ Push to the branch (`git push origin feature/amazing-feature`)
7. ğŸ”„ Open a Pull Request

## Requirements

- **Python 3.13+** for latest features and performance
- **Docker Desktop** with Compose V2 support
- **uv** package manager (recommended over pip)
- **NVIDIA GPU** (optional, for acceleration)
- **4GB+ RAM** recommended for model processing

## Compatibility

- âœ… **Windows 10/11** (WSL2 recommended for Docker)
- âœ… **macOS** (Intel and Apple Silicon)
- âœ… **Linux** (Ubuntu 20.04+, Fedora, Arch)
- âœ… **Docker Desktop** or **Docker Engine**
- âœ… **VS Code** with Dev Containers extension

## Docker Setup

### Quick Start
```bash
# ğŸš€ One-command setup
docker compose up -d

# ğŸ” Verify services
docker compose ps
```

### GPU Configuration
```bash
# ğŸ® GPU-enabled setup
docker compose -f docker-compose.gpu.yml up -d

# âœ… Test GPU access
docker exec -it equilens-ollama-1 nvidia-smi
```

## License

This project is licensed under the **Apache License 2.0** - see the [LICENSE.md](LICENSE.md) file for details.

### ğŸ¯ Quick License Summary
- âœ… **Commercial use allowed**
- âœ… **Modification and distribution permitted**
- âœ… **Patent protection included**
- ğŸ“‹ **Attribution required**
- ğŸ›¡ï¸ **No warranty provided**

## ğŸ‰ Success Story

EquiLens has evolved from a complex multi-script system to a streamlined, production-ready platform:

- ğŸ¨ **Enhanced User Experience**: Interactive CLI with Rich UI
- ğŸ”§ **Simplified Workflow**: Auto-discovery and guided setup
- âš¡ **Performance Optimized**: GPU acceleration and efficient processing
- ğŸ“ **Organized Output**: Session-based file management
- ğŸ³ **Container Ready**: Docker integration with GPU support
- ğŸ“Š **Professional Results**: Comprehensive bias analysis and reporting

**Ready to detect AI bias?** Start with `uv run equilens --help` and experience the difference! ğŸš€

---

>**ğŸ’¡ Pro Tip**: Start with `uv run equilens status` to check your system, then `uv run equilens start` to begin!
